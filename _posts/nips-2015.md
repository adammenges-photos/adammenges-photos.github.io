---
layout:     post
title:      NIPS 2015 Takeaways
date:       2015-12-25
summary:    This wasn't my first NIPS, but it was by far the largest.
permalink:  /nips-2015/
---

![live photo for the win](/images/nips2015-2.gif)

This wasn't my first NIPS (I grew up in Denver, after all), but it was by far the largest. The fact that machine learning has made it's way up to being written about in the NYT really showed this year. People were everywhere. The deep learning sessions were crowded.

![](/images/NIPS15growth.png)

Below is a random sampling of some highlights. Gaussian, of course.

### Saddle points

The saddle point problem was mentioned a lot, more then previous years. Although not from this year, [this paper](http://arxiv.org/pdf/1406.2572v1.pdf) was referenced often.

### Deep Visual Analogy-Making

![](/images/Deep-Visual-Analogy-Making.png)

The presentation for [this paper](https://web.eecs.umich.edu/~honglak/nips2015-analogy.pdf) was pretty good, particularity because it was presented by someone who knew how to present, and because they had spent awhile on their keynote. This topic in general, as the name suggests, as visual. So having good visuals to go along here was awesome, and got many oohs and awws from the audience.

### Ladder

Ladder Networks were also talked about quite a bit, and they are certainly pretty cool. They are roughly, take a feedforward model which serves the supervised learning as the encoder. Add a decoder which can invert the mappings on each layer of the encoder, the supervised cost is then calculated using the corrupted encoder output and the target. Train the network in a semi-supervised setting with something like SGD. Find more details [here](http://arxiv.org/abs/1507.02672).

### Highway

The [highway network](http://arxiv.org/abs/1505.00387) was also making the rounds.

### Adversarial networks



### Reinforcement Learning

Ending on a cool note. Reinforcement Learning was generally what was talked about around the table / after parties. 
